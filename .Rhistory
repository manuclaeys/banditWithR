IsRewardAreBoolean = FALSE
)
#Performence metrics
taux_reward_lin_ucb = max(resVal_linucb$cum_rew_linucb_rejection_sampling_alloc)/length(resVal_linucb$cum_rew_linucb_rejection_sampling_alloc)
list_results_lin_ucb   = append( list_results_lin_ucb ,taux_reward_lin_ucb)
#Random
resVal_random <- uniform_bandit_object_evaluationRejectionSampling(visitor_reward = visitor_reward2, average = FALSE  )
#Performence metrics
taux_reward_random  = max(  resVal_random$cum_rew_uniform_bandit_allocRejectionSampling)/length(resVal_random$cum_rew_uniform_bandit_allocRejectionSampling)
list_results_random    = append( list_results_random  ,taux_reward_random )
i=i+1
}
mean(list_results_dba_ctree_ucb  )
mean(list_results_dba_lin_ucb)
mean(list_results_ctree_ucb)
mean(list_results_lin_ucb )
mean(list_results_random )
sd(list_results_dba_ctree_ucb  )
sd(list_results_dba_lin_ucb)
sd(list_results_ctree_ucb )
sd(list_results_lin_ucb )
sd(list_results_random)
library(partykit)
while(i < 10){
#Config 100 - 100
#learnset = learnset[sample(nrow(learnset)),]
#testset = testset[sample(nrow(testset)),]
#total = rbind(learnset,testset)
#config = 0.5
#Config 70 - 30
total = total[sample(1:nrow(total)),]
config = 0.3
visitor_reward = total[,c("A","B")]
dt = total[,c("presence_time_serie","time_spend_time_serie","connexion_time_time_serie")]
K=ncol(visitor_reward)
alpha = 1
#Parametrage
ctree_parameters_control=ctreeucb_parameters_control_default(dt,visitor_reward,
is_reward_are_boolean = TRUE,
alpha = alpha,
arm_for_learn = names(visitor_reward)[1],
learn_size = as.integer(nrow(dt) * config))
listSerie = c("presence_time_serie","time_spend_time_serie","connexion_time_time_serie")
listKCentroids=c(11,3,3)
resVal_dbactreeucb <- dbactreeucbRejectionSamplingBanditObjectEvaluation(dt,visitor_reward,K, listSerie, listKCentroids , ctree_parameters_control)
#Performence metrics
taux_reward_dba_ctree_ucb = max(resVal_dbactreeucb$cum_rew_dbactreeucb_rejection_sampling_alloc)/length(resVal_dbactreeucb$cum_rew_dbactreeucb_rejection_sampling_alloc)
list_results_dba_ctree_ucb  = append(list_results_dba_ctree_ucb,taux_reward_dba_ctree_ucb)
###### Comparatif avec DBALinUCB
IsRewardAreBoolean = TRUE
listCategorial=0
listInteger=0
resVal_dbalinucb  <- dbalinucbRejectionSamplingBanditObjectEvaluation(dt=dt,
visitor_reward=visitor_reward,
alpha=alpha, K=ncol(visitor_reward),
listSerie, listKCentroids ,
learn_size = as.integer(nrow(dt) * config),
IsRewardAreBoolean = IsRewardAreBoolean,
listCategorial=listCategorial , listInteger=listInteger)
#Performence metrics
taux_reward_dba_lin_ucb = max(resVal_dbalinucb$cum_rew_dbalinucb_rejection_sampling_alloc)/length(resVal_dbalinucb$cum_rew_dbalinucb_rejection_sampling_alloc)
list_results_dba_lin_ucb  = append(list_results_dba_lin_ucb,taux_reward_dba_lin_ucb)
# Comparaison without time series clustering
for(k in listSerie ){
dt[[paste("mean_",k,sep = "")]] <- sapply(dt[,k],mean)
dt[,k]= NULL
}
#ctreeucb
ctree_parameters_control=ctreeucb_parameters_control_default(dt,visitor_reward,
explanatory_variable = colnames(dt),
is_reward_are_boolean = TRUE,
alpha = 1,learn_size = as.integer(nrow(dt) * config))
resVal_ctreeucb <- ctreeucbRejectionSamplingBanditObjectEvaluation(dt,visitor_reward,K, ctree_parameters_control)
#Performence metrics
taux_reward_ctree_ucb = max(resVal_ctreeucb$cum_rew_ctreeucb_rejection_sampling_alloc)/length(resVal_ctreeucb$cum_rew_ctreeucb_rejection_sampling_alloc)
list_results_ctree_ucb  = append( list_results_ctree_ucb,taux_reward_ctree_ucb)
#udate data
dt2 = dt[resVal_ctreeucb$ctreeucb_rejection_sampling_bandit_alloc$first_train_element:nrow(dt),]
visitor_reward2 = visitor_reward[resVal_ctreeucb$ctreeucb_rejection_sampling_bandit_alloc$first_train_element:nrow(dt),]
#LinUCB
resVal_linucb <- LinucbRejectionSamplingBanditObjectEvaluation(
dt2,
visitor_reward = visitor_reward2,
K = ncol(visitor_reward),
alpha = 1,
IsRewardAreBoolean = FALSE
)
#Performence metrics
taux_reward_lin_ucb = max(resVal_linucb$cum_rew_linucb_rejection_sampling_alloc)/length(resVal_linucb$cum_rew_linucb_rejection_sampling_alloc)
list_results_lin_ucb   = append( list_results_lin_ucb ,taux_reward_lin_ucb)
#Random
resVal_random <- uniform_bandit_object_evaluationRejectionSampling(visitor_reward = visitor_reward2, average = FALSE  )
#Performence metrics
taux_reward_random  = max(  resVal_random$cum_rew_uniform_bandit_allocRejectionSampling)/length(resVal_random$cum_rew_uniform_bandit_allocRejectionSampling)
list_results_random    = append( list_results_random  ,taux_reward_random )
i=i+1
}
library(bandit4abtest)
library(bandit4abtest)
while(i < 10){
#Config 100 - 100
#learnset = learnset[sample(nrow(learnset)),]
#testset = testset[sample(nrow(testset)),]
#total = rbind(learnset,testset)
#config = 0.5
#Config 70 - 30
total = total[sample(1:nrow(total)),]
config = 0.3
visitor_reward = total[,c("A","B")]
dt = total[,c("presence_time_serie","time_spend_time_serie","connexion_time_time_serie")]
K=ncol(visitor_reward)
alpha = 1
#Parametrage
ctree_parameters_control=ctreeucb_parameters_control_default(dt,visitor_reward,
is_reward_are_boolean = TRUE,
alpha = alpha,
arm_for_learn = names(visitor_reward)[1],
learn_size = as.integer(nrow(dt) * config))
listSerie = c("presence_time_serie","time_spend_time_serie","connexion_time_time_serie")
listKCentroids=c(11,3,3)
resVal_dbactreeucb <- dbactreeucbRejectionSamplingBanditObjectEvaluation(dt,visitor_reward,K, listSerie, listKCentroids , ctree_parameters_control)
#Performence metrics
taux_reward_dba_ctree_ucb = max(resVal_dbactreeucb$cum_rew_dbactreeucb_rejection_sampling_alloc)/length(resVal_dbactreeucb$cum_rew_dbactreeucb_rejection_sampling_alloc)
list_results_dba_ctree_ucb  = append(list_results_dba_ctree_ucb,taux_reward_dba_ctree_ucb)
###### Comparatif avec DBALinUCB
IsRewardAreBoolean = TRUE
listCategorial=0
listInteger=0
resVal_dbalinucb  <- dbalinucbRejectionSamplingBanditObjectEvaluation(dt=dt,
visitor_reward=visitor_reward,
alpha=alpha, K=ncol(visitor_reward),
listSerie, listKCentroids ,
learn_size = as.integer(nrow(dt) * config),
IsRewardAreBoolean = IsRewardAreBoolean,
listCategorial=listCategorial , listInteger=listInteger)
#Performence metrics
taux_reward_dba_lin_ucb = max(resVal_dbalinucb$cum_rew_dbalinucb_rejection_sampling_alloc)/length(resVal_dbalinucb$cum_rew_dbalinucb_rejection_sampling_alloc)
list_results_dba_lin_ucb  = append(list_results_dba_lin_ucb,taux_reward_dba_lin_ucb)
# Comparaison without time series clustering
for(k in listSerie ){
dt[[paste("mean_",k,sep = "")]] <- sapply(dt[,k],mean)
dt[,k]= NULL
}
#ctreeucb
ctree_parameters_control=ctreeucb_parameters_control_default(dt,visitor_reward,
explanatory_variable = colnames(dt),
is_reward_are_boolean = TRUE,
alpha = 1,learn_size = as.integer(nrow(dt) * config))
resVal_ctreeucb <- ctreeucbRejectionSamplingBanditObjectEvaluation(dt,visitor_reward,K, ctree_parameters_control)
#Performence metrics
taux_reward_ctree_ucb = max(resVal_ctreeucb$cum_rew_ctreeucb_rejection_sampling_alloc)/length(resVal_ctreeucb$cum_rew_ctreeucb_rejection_sampling_alloc)
list_results_ctree_ucb  = append( list_results_ctree_ucb,taux_reward_ctree_ucb)
#udate data
dt2 = dt[resVal_ctreeucb$ctreeucb_rejection_sampling_bandit_alloc$first_train_element:nrow(dt),]
visitor_reward2 = visitor_reward[resVal_ctreeucb$ctreeucb_rejection_sampling_bandit_alloc$first_train_element:nrow(dt),]
#LinUCB
resVal_linucb <- LinucbRejectionSamplingBanditObjectEvaluation(
dt2,
visitor_reward = visitor_reward2,
K = ncol(visitor_reward),
alpha = 1,
IsRewardAreBoolean = FALSE
)
#Performence metrics
taux_reward_lin_ucb = max(resVal_linucb$cum_rew_linucb_rejection_sampling_alloc)/length(resVal_linucb$cum_rew_linucb_rejection_sampling_alloc)
list_results_lin_ucb   = append( list_results_lin_ucb ,taux_reward_lin_ucb)
#Random
resVal_random <- uniform_bandit_object_evaluationRejectionSampling(visitor_reward = visitor_reward2, average = FALSE  )
#Performence metrics
taux_reward_random  = max(  resVal_random$cum_rew_uniform_bandit_allocRejectionSampling)/length(resVal_random$cum_rew_uniform_bandit_allocRejectionSampling)
list_results_random    = append( list_results_random  ,taux_reward_random )
i=i+1
}
rm(list = ls())
#Contextual with continus reward
size.tot = 10000
set.seed(4649)                          # this makes the example exactly reproducible
x1 = runif(size.tot, min=0, max=10)          # you have 2, largely uncorrelated predictors
x2 = runif(size.tot, min=0, max=10)
dt = cbind(x1,x2)
#arm reward
arm_1 <-  as.vector(c(0,0.5))
K1 = crossprod(t(dt),arm_1) + runif(size.tot, min=-1, max=1)    #  linear predictor
summary(K1)
arm_2 <-  as.vector(c(0.5,0))
K2 = crossprod(t(dt),arm_2) + runif(size.tot, min=-1, max=1)
summary(K2)
visitor_reward <-  data.frame(K1,K2)
dt <- as.data.frame(dt)
is.na(dt)
length(NA)
dt == NA
df == []
dim(df) == NULL
dim(NA) == NULL
toto = NA
toto[1]
toto[[1]]
df[[i]]
df[[1]]
df[1]
df[1,]
dt[[1]]
dt[1,1]
toto[1,1]
dt==0
dim(dt)
dim(NA)
dim(dt) ==NULL
dim(dt) !=NULL
dim(dt)[1] !=NULL
dim(dt)[1]
dim(dt)[1] == NULL
dim(toto)
if(dim(toto) == NULL) print("putain")
if(toto == NULL) print("putain")
if(toto == NA) print("putain")
dt == 0
if(dt == 0) print("merde")
toto = c(0,0)
toto == c(0,0)
if(toto) print("coucou")
dim(dt)
dim(dt)[2]
dim(toto)[2]
if(dim(toto)[2]>0)
print("yp")
dt == toto
toto == toto
toto[1] == toto[1]
toto[1] == 0
dt[1] == 0
length(unlist(dt))
length(dt)
length(NA)
library(bandit4abtest)
devtools::load_all(".")
rm(list = ls())
#Contextual with continus reward
size.tot = 10000
set.seed(4649)                          # this makes the example exactly reproducible
x1 = runif(size.tot, min=0, max=10)          # you have 2, largely uncorrelated predictors
x2 = runif(size.tot, min=0, max=10)
dt = cbind(x1,x2)
#arm reward
arm_1 <-  as.vector(c(0,0.5))
K1 = crossprod(t(dt),arm_1) + runif(size.tot, min=-1, max=1)    #  linear predictor
summary(K1)
arm_2 <-  as.vector(c(0.5,0))
K2 = crossprod(t(dt),arm_2) + runif(size.tot, min=-1, max=1)
summary(K2)
visitor_reward <-  data.frame(K1,K2)
dt <- as.data.frame(dt)
linucb_contextual_alloc <- LINUCB(dt,visitor_reward )
cum_reg_linucb_contextual_alloc <- cumulativeRegretAverage(linucb_contextual_alloc$choice,visitor_reward,dt = dt)
library(devtools)
install_github("https://github.com/manuclaeys/bandit4abtest")
library(bandit4abtest)
set.seed(4434)
V1 <- rbinom(5000, 1, 0.6)  # Generates 5000 numbers from a uniform distribution with mean 0.75
V2 <- rbinom(5000, 1, 0.7)
V3 <- rbinom(5000, 1, 0.5)
V4 <- rbinom(5000, 1, 0.3)
V5 <- rbinom(5000, 1, 0.9)
visitorReward <- as.data.frame( cbind(V1,V2,V3,V4,V5) )
ucb_alloc  <- UCB(visitorReward,alpha = 1)
cum_reg_ucb_alloc  <- cumulativeRegretAverage(ucb_alloc$choice,visitorReward)
epsilonGreedy_alloc <- EpsilonGreedy(visitorReward,epsilon  = 0.05)
cum_reg_epsilonGreedy_alloc  <- cumulativeRegretAverage(epsilonGreedy_alloc$choice,visitorReward)
thompson_sampling_alloc <- ThompsonSampling(visitorReward)
cum_reg_thompson_sampling_alloc <- cumulativeRegretAverage(thompson_sampling_alloc$choice,visitorReward)
random_alloc <- UniformBandit(visitorReward)
cum_reg_random_alloc <- cumulativeRegretAverage(random_alloc$choice,visitorReward)
#################
rm(list = ls())
#Contextual with continus reward
size.tot = 10000
set.seed(4649)                          # this makes the example exactly reproducible
x1 = runif(size.tot, min=0, max=10)          # you have 2, largely uncorrelated predictors
x2 = runif(size.tot, min=0, max=10)
dt = cbind(x1,x2)
#arm reward
arm_1 <-  as.vector(c(0,0.5))
K1 = crossprod(t(dt),arm_1) + runif(size.tot, min=-1, max=1)    #  linear predictor
summary(K1)
arm_2 <-  as.vector(c(0.5,0))
K2 = crossprod(t(dt),arm_2) + runif(size.tot, min=-1, max=1)
summary(K2)
visitor_reward <-  data.frame(K1,K2)
dt <- as.data.frame(dt)
linucb_contextual_alloc <- LINUCB(dt,visitor_reward )
cum_reg_linucb_contextual_alloc <- cumulativeRegretAverage(linucb_contextual_alloc$choice,visitor_reward,dt = dt)
#######
library(MASS)
model1 <- glm(type~.,family=binomial(link="logit"),data=Pima.tr)
proba.logit <- predict(model1,Pima.te,type="response")
classif.logit <- proba.logit>=1/2
classif.logit[classif.logit==TRUE] <- "Yes"
classif.logit[classif.logit==FALSE] <- "No"
mean(classif.logit!=Pima.te$type)
model2 <- glm(type~.,family=binomial(link="probit"),data=Pima.tr)
proba.probit <- predict(model2,Pima.te,type="response")
classif.probit <- proba.probit >=1/2
classif.probit[classif.probit==TRUE] <- "Yes"
classif.probit[classif.probit==FALSE] <- "No"
mean(classif.probit!=Pima.te$type)
curve(plogis,-10,10)
curve(pnorm,-10,10,col="red",add=TRUE)
mydata = Pima.tr
View(mydata)
View(mydata)
convert_diab  <- mydata %>%
mutate(type = ifelse(type == "No",0,1))
require(dplyr)
convert_diab  <- mydata %>%
mutate(type = ifelse(type == "No",0,1))
View(convert_diab)
curve(plogis,mydata$glu)
curve(pnorm,,mydata$glu,col="red",add=TRUE)
curve(plogis,mydata$glu)
curve(pnorm,mydata$glu,col="red",add=TRUE)
curve(pnorm,col="red",add=TRUE)
summary(model2 )
summary(model1 )
remove(list = ls())
set.seed(1234)
library(partykit)
#Importe data
library(jsonlite)
library(readr)
data.train  <- jsonlite::fromJSON("parcoursuserDatabaseFinalModz1", simplifyDataFrame = TRUE)
#visitorReward <- jsonlite::fromJSON("parcours_reward_modz.JSON", simplifyDataFrame = TRUE)
visitorReward <- read.csv2("~/Documents/Experimentation/dbactreeucb/ModzLeaveAtLeastOne.csv", header = TRUE, sep = ',',colClasses=c("fullVisitorId"="character"))
visitorReward$X = NULL
visitorReward$A = visitorReward$A*100
visitorReward$B = visitorReward$B*100
total <- merge(data.train ,visitorReward, by="fullVisitorId")
library(bandit4abtest)
library(partykit)
####Configuration
#Conf_30/70
config <- "30_70"
df <- abtest1
df$langID <- as.factor(df$langID)
df$countryID <- as.factor(df$countryID)
listCategorial =c("countryID","langID","name","device","userAgent")
listInteger  = c("latitude","longitude")
#Results for each variation
visitorReward <- df[,c("A","B")]
#Items caracteristics
dt <- df[, c(listCategorial,listInteger)]
set.seed(1234)
if(config  == "100_100" ) learn_size = 6216
if(config  ==  "30_70"  ) learn_size = 1865
#### replication ###
rep_data <- 2
dt <- rbind(dt,as.data.frame(lapply(dt[(learn_size+1):nrow(dt),],function(x)rep(x,rep_data ))) )
visitorReward <- rbind(visitorReward,as.data.frame(lapply(visitorReward[(learn_size+1):nrow(visitorReward),],function(x)rep(x,rep_data ))) )
dt.old <- dt
ctreeucb_parameters_control <- ctreeucb_parameters_control_default(dt = dt.old,
visitorReward ,
learn_size = learn_size,
alpha = 1,
arm_for_learn = names(visitorReward)[1],
is_reward_are_boolean = TRUE,
ctree_control_val=ctree_control(
mincriterion = 0.95,
testtype = "Bonferroni",
teststat = "quadratic",
splitstat = c( "quadratic"))
)
my_ctree_ucb <- ctreeucbBanditObjectEvaluation(dt= dt.old,visitor_reward=visitorReward, ctree_parameters_control= ctreeucb_parameters_control, average = TRUE)
max(my_ctree_ucb$cum_reg_ctree)
library(bandit4abtest)
set.seed(4434)
V1 <- rbinom(5000, 1, 0.6)  # Generates 5000 numbers from a uniform distribution with mean 0.75
V2 <- rbinom(5000, 1, 0.7)
V3 <- rbinom(5000, 1, 0.5)
V4 <- rbinom(5000, 1, 0.3)
V5 <- rbinom(5000, 1, 0.9)
visitorReward <- as.data.frame( cbind(V1,V2,V3,V4,V5) )
ucb_alloc  <- UCB(visitorReward,alpha = 1)
cum_reg_ucb_alloc  <- cumulativeRegretAverage(ucb_alloc$choice,visitorReward)
epsilonGreedy_alloc <- EpsilonGreedy(visitorReward,epsilon  = 0.05)
cum_reg_epsilonGreedy_alloc  <- cumulativeRegretAverage(epsilonGreedy_alloc$choice,visitorReward)
thompson_sampling_alloc <- ThompsonSampling(visitorReward)
cum_reg_thompson_sampling_alloc <- cumulativeRegretAverage(thompson_sampling_alloc$choice,visitorReward)
random_alloc <- UniformBandit(visitorReward)
cum_reg_random_alloc <- cumulativeRegretAverage(random_alloc$choice,visitorReward)
library(ggplot2)
comp_reg <- data.frame(cbind(cum_reg_ucb_alloc,
cum_reg_epsilonGreedy_alloc,
cum_reg_thompson_sampling_alloc,
cum_reg_random_alloc ))
ggplot(comp_reg, aes(c(1:nrow(comp_reg)), y = value, color = Algorithm)) +
geom_line(linetype="dashed",aes(y = cum_reg_ucb_alloc, col = "UCB"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_epsilonGreedy_alloc, col = "Epsilon Greedy"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_thompson_sampling_alloc, col = "Thompson Sampling"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_random_alloc, col = "Random"),size = 0.5) +
scale_colour_manual(values =  c("UCB"="brown","Epsilon Greedy"="orange","Thompson Sampling"="green","Random"="black"))+
xlab("Time T") +
ylab("Cumulative regret")
#################
rm(list = ls())
#Contextual with continus reward
size.tot = 10000
set.seed(4649)                          # this makes the example exactly reproducible
x1 = runif(size.tot, min=0, max=10)          # you have 2, largely uncorrelated predictors
x2 = runif(size.tot, min=0, max=10)
dt = cbind(x1,x2)
#arm reward
arm_1 <-  as.vector(c(0,0.5))
K1 = crossprod(t(dt),arm_1) + runif(size.tot, min=-1, max=1)    #  linear predictor
summary(K1)
arm_2 <-  as.vector(c(0.5,0))
K2 = crossprod(t(dt),arm_2) + runif(size.tot, min=-1, max=1)
summary(K2)
visitor_reward <-  data.frame(K1,K2)
dt <- as.data.frame(dt)
linucb_contextual_alloc <- LINUCB(dt,visitor_reward )
library(pracma)
library(matlib)
linucb_contextual_alloc <- LINUCB(dt,visitor_reward )
cum_reg_linucb_contextual_alloc <- cumulativeRegretAverage(linucb_contextual_alloc$choice,visitor_reward,dt = dt)
kernelucb_contextual_alloc <- kernelucb(dt, visitor_reward, update_val = 500)
random_alloc <- UniformBandit(visitor_reward)
cum_reg_random_alloc <- cumulativeRegretAverage(random_alloc$choice,visitor_reward,dt=dt)
ucb_alloc  <- UCB( visitor_reward,alpha = 1)
cum_reg_ucb_alloc  <- cumulativeRegretAverage(ucb_alloc$choice, visitor_reward,dt=dt)
epsilonGreedy_alloc <- EpsilonGreedy( visitor_reward,epsilon  = 0.05)
cum_reg_epsilonGreedy_alloc  <- cumulativeRegretAverage(epsilonGreedy_alloc$choice,visitor_reward,dt=dt)
library(ggplot2)
comp_reg <- data.frame(cbind(cum_reg_linucb_contextual_alloc,
cum_reg_kernelucb_contextual_alloc,
cum_reg_random_alloc,
cum_reg_ucb_alloc,
cum_reg_epsilonGreedy_alloc))
ggplot(comp_reg, aes(c(1:nrow(comp_reg)), y = value, color = Algorithm)) +
geom_line(linetype="dashed",aes(y = cum_reg_ucb_alloc, col = "UCB"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_epsilonGreedy_alloc, col = "Epsilon Greedy"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_random_alloc, col = "Random"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_linucb_contextual_alloc, col = "LINUCB"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_kernelucb_contextual_alloc, col = "KernelUCB"),size = 0.5) +
scale_colour_manual(values =  c("UCB"="brown","Epsilon Greedy"="orange","Thompson Sampling"="green","Random"="black", "Contextual TS"="dark green", "LINUCB"="blue","KernelUCB"= "purple"))+
xlab("Time T") +
ylab("Cumulative regret")
#################
rm(list = ls())
##### None linear #####
rm(list = ls())
##### Pairewise #####
set.seed(1234)
size.tot <- 10000
x <- seq(0, 5, 0.01)
x1<- sample(x, size.tot, replace = TRUE, prob = NULL)
arm_1 <-  as.vector(c(2,-1,1.5,0))
K1 <- (x1 < 1 ) * arm_1[4]  +
(x1 >= 1 & x1 < 2 ) * arm_1[1]  +
(x1 >= 2 & x1 < 3) * arm_1[2]  +
(x1 >= 3 & x1 < 4) * arm_1[3]  +
(x1 >= 4) * arm_1[4]
plot(x1, K1)
arm_2 <-  as.vector(c(1.5,-0.5,1.25,0))
K2 <- (x1 < 1 ) * arm_2[4]  +
(x1 >= 1 & x1 < 2 ) * arm_2[1]  +
(x1 >= 2 & x1 < 3) * arm_2[2]  +
(x1 >= 3 & x1 < 4) * arm_2[3]  +
(x1 >= 4) * arm_2[4]
plot(x1, K2)
#covariate without interest
x2<- sample(x, size.tot, replace = TRUE, prob = NULL)
#Results for each variation
visitor_reward <-  data.frame(K1,K2 )
summary(visitor_reward)
dt <- as.data.frame(cbind(x1,x2))
controle_param = ctreeucb_parameters_control_default(dt=dt, visitor_reward=visitor_reward,learn_size=1500,  alpha=1, ctree_control_val= partykit::ctree_control(teststat = "quadratic"))
ctreeucb_alloc <- ctreeucb(dt, visitor_reward, ctree_parameters_control = controle_param )
#take data for online ab test for other algorithm
first <- ctreeucb_alloc$first_train_element
last <- nrow(visitor_reward)
dt.abtest <- dt[first:last,]
visitor_reward.abtest <- visitor_reward[first:last,]
cum_reg_ctreeucb_alloc <- cumulativeRegret(ctreeucb_alloc$choice,visitor_reward.abtest)
linucb_contextual_alloc <- LINUCB(dt.abtest,visitor_reward.abtest )
cum_reg_linucb_contextual_alloc <- cumulativeRegretAverage(linucb_contextual_alloc$choice,visitor_reward.abtest,dt = dt.abtest)
kernelucb_contextual_alloc <- kernelucb(dt.abtest, visitor_reward.abtest, update_val = 500)
cum_reg_kernelucb_contextual_alloc <- cumulativeRegretAverage(kernelucb_contextual_alloc$choice,visitor_reward.abtest,dt=dt.abtest)
max(cum_reg_kernelucb_contextual_alloc)
random_alloc <- UniformBandit(visitor_reward.abtest)
cum_reg_random_alloc <- cumulativeRegret(random_alloc$choice,visitor_reward.abtest)
ucb_alloc  <- UCB( visitor_reward.abtest,alpha = 1)
cum_reg_ucb_alloc  <- cumulativeRegret(ucb_alloc$choice, visitor_reward.abtest)
epsilonGreedy_alloc <- EpsilonGreedy( visitor_reward.abtest,epsilon  = 0.05)
cum_reg_epsilonGreedy_alloc  <- cumulativeRegret(epsilonGreedy_alloc$choice,visitor_reward.abtest)
library(ggplot2)
comp_reg <- data.frame(cbind(cum_reg_linucb_contextual_alloc,
cum_reg_kernelucb_contextual_alloc,
cum_reg_random_alloc,
cum_reg_ucb_alloc,
cum_reg_epsilonGreedy_alloc,
cum_reg_ctreeucb_alloc))
ggplot(comp_reg, aes(c(1:nrow(comp_reg)), y = value, color = Algorithm)) +
geom_line(linetype="dashed",aes(y = cum_reg_ucb_alloc, col = "UCB"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_epsilonGreedy_alloc, col = "Epsilon Greedy"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_random_alloc, col = "Random"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_linucb_contextual_alloc, col = "LINUCB"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_kernelucb_contextual_alloc, col = "KernelUCB"),size = 0.5) +
geom_line(linetype="dashed",aes(y = cum_reg_ctreeucb_alloc, col = "CtreeUCB"),size = 0.5) +
scale_colour_manual(values =  c("UCB"="brown","Epsilon Greedy"="orange","Random"="black","LINUCB"="blue","KernelUCB"= "purple" ,"CtreeUCB"="red"))+
xlab("Time T") +
ylab("Cumulative regret")
set.seed(1234)
